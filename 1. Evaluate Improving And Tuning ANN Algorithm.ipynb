{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "# Spliting input and output attributes\n",
    "array=dataset.values\n",
    "x=array[:,3:-1]\n",
    "y=array[:,-1]\n",
    "\n",
    "# Encode catergorical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_1=LabelEncoder()\n",
    "x[:,1]=labelencoder_1.fit_transform(x[:,1])\n",
    "labelencoder_2=LabelEncoder()\n",
    "x[:,2]=labelencoder_2.fit_transform(x[:,2])\n",
    "\n",
    "# Encoding numerical values as dummy variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder=OneHotEncoder(categorical_features=[1])\n",
    "x=onehotencoder.fit_transform(x).toarray()\n",
    "\n",
    "# Handling Dummy variable trap\n",
    "x=x[:,1:]\n",
    "\n",
    "# Normalization Scale\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaled_value=Normalizer()\n",
    "x=scaled_value.fit_transform(x)\n",
    "\n",
    "# Split the dataset into train and test set\n",
    "train_size=0.80\n",
    "test_size=0.20\n",
    "seed=5\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8075     0.7925     0.8175     0.76375    0.79875    0.80624999\n",
      " 0.78125    0.775      0.81125    0.80625   ]\n",
      "0.7959999971464276\n",
      "0.016600074941073958\n"
     ]
    }
   ],
   "source": [
    "# Evalute The ANN Algrithm using KFold Technique to minimize variance\n",
    "# Below library we are using to make connect betwenn keras framework to scikit_learn library\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "nodes=len(x_train[0])\n",
    "halfnode=int(nodes+1/2)\n",
    "def build_ann():\n",
    "    ann=Sequential()\n",
    "    ann.add(Dense(output_dim=halfnode,input_dim=nodes,activation='relu',init='uniform'))\n",
    "    ann.add(Dense(output_dim=halfnode,init='uniform',activation='relu'))\n",
    "    ann.add(Dense(output_dim=halfnode,init='uniform',activation='relu'))\n",
    "    ann.add(Dense(output_dim=1,activation='sigmoid',init='uniform'))\n",
    "    ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return ann\n",
    "ann=KerasClassifier(build_fn=build_ann,batch_size=10,nb_epoch=100)\n",
    "predictions=cross_val_score(estimator=ann,X=x_train,y=y_train,cv=10,n_jobs=-1)\n",
    "mean_Accuracy=predictions.mean()\n",
    "variance_mean_accuracy=predictions.std()\n",
    "print(predictions)\n",
    "print(mean_Accuracy)\n",
    "print(variance_mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8075     0.7925     0.8175     0.76375    0.79875    0.80624999\n",
      " 0.78125    0.775      0.81125    0.80625   ]\n",
      "0.7959999971464276\n",
      "0.016600074941073958\n"
     ]
    }
   ],
   "source": [
    "# Improve Ann Algorithm from over and underfitting to trainset using dropout class\n",
    "\n",
    "\n",
    "# dropout class if used to disable certain nodes while training the model it range from 0 to 1 means 0.1=10% to 1=100%\n",
    "# we need to use in between 0.1 to 0.4 value if we use above 0.5 it will be under fit.\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nodes=len(x_train[0])\n",
    "halfnode=int(nodes+1/2)\n",
    "def build_ann():\n",
    "    ann=Sequential()\n",
    "    ann.add(Dense(output_dim=halfnode,input_dim=nodes,init='uniform',activation='relu'))\n",
    "    # It will drop 10% nodes in a layer to prevent overfitting\n",
    "    ann.add(Dropout(p=0.1))\n",
    "    ann.add(Dense(output_dim=halfnode,init='uniform',activation='relu'))\n",
    "    ann.add(Dropout(p=0.1))\n",
    "    ann.add(Dense(output_dim=halfnode,init='uniform',activation='relu'))\n",
    "    ann.add(Dropout(p=0.1))\n",
    "    ann.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return ann\n",
    "ann=KerasClassifier(build_fn=build_ann,batch_size=10,nb_epoch=100)\n",
    "predictions=cross_val_score(estimator=ann,X=x_train,y=y_train,cv=10,n_jobs=-1)\n",
    "mean_predictions=predictions.mean()\n",
    "mean_variance_predicitons=predictions.std()\n",
    "print(predictions)\n",
    "print(mean_predictions)\n",
    "print(mean_variance_predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=11, activation=\"relu\", units=11, kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n",
      "  \n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\arunkumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 63s 9ms/step - loss: 0.5356 - acc: 0.7943\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 57s 8ms/step - loss: 0.5310 - acc: 0.7964\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 55s 8ms/step - loss: 0.5402 - acc: 0.7933\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 59s 8ms/step - loss: 0.5371 - acc: 0.7990\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 56s 8ms/step - loss: 0.5367 - acc: 0.7946\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 56s 8ms/step - loss: 0.5352 - acc: 0.7949\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 57s 8ms/step - loss: 0.5299 - acc: 0.7971\n",
      "800/800 [==============================] - 15s 19ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 58s 8ms/step - loss: 0.5357 - acc: 0.7981\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 58s 8ms/step - loss: 0.5378 - acc: 0.7943\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 65s 9ms/step - loss: 0.5318 - acc: 0.7947\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 60s 8ms/step - loss: 0.5374 - acc: 0.7947\n",
      "800/800 [==============================] - 17s 21ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 61s 9ms/step - loss: 0.5331 - acc: 0.7963\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 62s 9ms/step - loss: 0.5432 - acc: 0.7925\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 63s 9ms/step - loss: 0.5267 - acc: 0.7996: 5s - loss:\n",
      "800/800 [==============================] - 17s 21ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 61s 9ms/step - loss: 0.5306 - acc: 0.7957\n",
      "800/800 [==============================] - 17s 21ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 62s 9ms/step - loss: 0.5396 - acc: 0.7947\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 370s 51ms/step - loss: 0.5312 - acc: 0.7969\n",
      "800/800 [==============================] - 10s 13ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 34s 5ms/step - loss: 0.5304 - acc: 0.7983\n",
      "800/800 [==============================] - 10s 13ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 53s 7ms/step - loss: 0.5447 - acc: 0.7932\n",
      "800/800 [==============================] - 16s 20ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 66s 9ms/step - loss: 0.5341 - acc: 0.7949\n",
      "800/800 [==============================] - 17s 21ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 65s 9ms/step - loss: 0.5376 - acc: 0.7946\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 63s 9ms/step - loss: 0.5321 - acc: 0.7957\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 70s 10ms/step - loss: 0.5386 - acc: 0.7929\n",
      "800/800 [==============================] - 20s 25ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 65s 9ms/step - loss: 0.5263 - acc: 0.7993\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 66s 9ms/step - loss: 0.5390 - acc: 0.7956\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 64s 9ms/step - loss: 0.5350 - acc: 0.7944\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 68s 9ms/step - loss: 0.5295 - acc: 0.7976\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 65s 9ms/step - loss: 0.5369 - acc: 0.7978\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 68s 9ms/step - loss: 0.5434 - acc: 0.7940\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 65s 9ms/step - loss: 0.5392 - acc: 0.7943\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 69s 10ms/step - loss: 0.5373 - acc: 0.7942\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 71s 10ms/step - loss: 0.5312 - acc: 0.7961\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 71s 10ms/step - loss: 0.5367 - acc: 0.7931\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 68s 9ms/step - loss: 0.5321 - acc: 0.7992\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 66s 9ms/step - loss: 0.5361 - acc: 0.7954\n",
      "800/800 [==============================] - 20s 25ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 69s 10ms/step - loss: 0.5366 - acc: 0.7946\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 68s 9ms/step - loss: 0.5344 - acc: 0.7972\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 71s 10ms/step - loss: 0.5374 - acc: 0.7978\n",
      "800/800 [==============================] - 20s 25ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 73s 10ms/step - loss: 0.5319 - acc: 0.7943\n",
      "800/800 [==============================] - 22s 28ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 72s 10ms/step - loss: 0.5384 - acc: 0.7943\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 73s 10ms/step - loss: 0.5408 - acc: 0.7940\n",
      "800/800 [==============================] - 21s 26ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 69s 10ms/step - loss: 0.5342 - acc: 0.7954\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 72s 10ms/step - loss: 0.5368 - acc: 0.7933\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 70s 10ms/step - loss: 0.5302 - acc: 0.7992\n",
      "800/800 [==============================] - 20s 25ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 0.5326 - acc: 0.7953\n",
      "800/800 [==============================] - 25s 31ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 89s 12ms/step - loss: 0.5385 - acc: 0.7949\n",
      "800/800 [==============================] - 21s 27ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 0.5363 - acc: 0.7968\n",
      "800/800 [==============================] - 23s 28ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 87s 12ms/step - loss: 0.5394 - acc: 0.7975\n",
      "800/800 [==============================] - 23s 29ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 92s 13ms/step - loss: 0.5381 - acc: 0.7939\n",
      "800/800 [==============================] - 23s 29ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 85s 12ms/step - loss: 0.5403 - acc: 0.7938\n",
      "800/800 [==============================] - 23s 29ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 93s 13ms/step - loss: 0.5360 - acc: 0.7946\n",
      "800/800 [==============================] - 23s 28ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 82s 11ms/step - loss: 0.5291 - acc: 0.7964\n",
      "800/800 [==============================] - 21s 26ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 87s 12ms/step - loss: 0.5364 - acc: 0.7935\n",
      "800/800 [==============================] - 21s 26ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 81s 11ms/step - loss: 0.5293 - acc: 0.7996\n",
      "800/800 [==============================] - 21s 26ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10965s 2s/step - loss: 0.5399 - acc: 0.7953\n",
      "800/800 [==============================] - 13s 16ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 45s 6ms/step - loss: 0.5348 - acc: 0.7946\n",
      "800/800 [==============================] - 13s 16ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 69s 10ms/step - loss: 0.5277 - acc: 0.7976\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 70s 10ms/step - loss: 0.5297 - acc: 0.7983\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 72s 10ms/step - loss: 0.5338 - acc: 0.7940\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 70s 10ms/step - loss: 0.5393 - acc: 0.7942\n",
      "800/800 [==============================] - 18s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 73s 10ms/step - loss: 0.5372 - acc: 0.7939\n",
      "800/800 [==============================] - 19s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 71s 10ms/step - loss: 0.5375 - acc: 0.7961\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 74s 10ms/step - loss: 0.5354 - acc: 0.7931\n",
      "800/800 [==============================] - 19s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 73s 10ms/step - loss: 0.5261 - acc: 0.7994\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 74s 10ms/step - loss: 0.5326 - acc: 0.7956\n",
      "800/800 [==============================] - 19s 23ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 74s 10ms/step - loss: 0.5356 - acc: 0.7946\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10942s 2s/step - loss: 0.5268 - acc: 0.7976\n",
      "800/800 [==============================] - 13s 17ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 60s 8ms/step - loss: 0.5293 - acc: 0.7985\n",
      "800/800 [==============================] - 19s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 75s 10ms/step - loss: 0.5381 - acc: 0.7940\n",
      "800/800 [==============================] - 20s 24ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 75s 10ms/step - loss: 0.5414 - acc: 0.7946\n",
      "800/800 [==============================] - 20s 25ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7201s 1s/step - loss: 0.5401 - acc: 0.7947\n",
      "800/800 [==============================] - 18s 22ms/step\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 58s 8ms/step - loss: 0.5348 - acc: 0.7961\n",
      "800/800 [==============================] - 21s 26ms/step\n",
      "Epoch 1/1\n",
      "5544/7200 [======================>.......] - ETA: 27s - loss: 0.5491 - acc: 0.7942"
     ]
    }
   ],
   "source": [
    "# Tunning ANN Algorithm to check hyperparameter to find best combination\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nodes=len(x_train[0])\n",
    "half=int(nodes+1/2)\n",
    "def build_ann(optimizer):\n",
    "    ann=Sequential()\n",
    "    ann.add(Dense(output_dim=half,input_dim=nodes,init='uniform',activation='relu'))\n",
    "    ann.add(Dropout(p=0.2))\n",
    "    ann.add(Dense(output_dim=half,init='uniform',activation='relu'))\n",
    "    ann.add(Dropout(p=0.2))\n",
    "    ann.add(Dense(output_dim=half,init='uniform',activation='relu'))\n",
    "    ann.add(Dropout(p=0.2))\n",
    "    ann.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\n",
    "    ann.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return ann\n",
    "ann=KerasClassifier(build_fn=build_ann)\n",
    "params={'optimizer':['adam','rmsprop'],\n",
    "        'batch_size':[8,16,32],\n",
    "        'nb_epoch':[50,100,150,200]}\n",
    "results=GridSearchCV(estimator=ann,\n",
    "                    param_grid=params,\n",
    "                    cv=10)\n",
    "grid_search=results.fit(x_train,y_train)\n",
    "best_parameter=grid_search.best_params_\n",
    "best_accuracy=grid_search.best_score_\n",
    "print(best_parameter)\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we will get perfect combination of hyper parameters values\n",
    "# We can choose those values for test data predictions\n",
    "# I was abort excution because it is taking more thank a day in my local machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
